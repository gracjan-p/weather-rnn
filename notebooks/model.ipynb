{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "In this small project we will take a look at Seattle weather dataset from Kaggle to extract important features and use them to test functionality of Recurrent Neural Network.\n",
    "<br><br>\n",
    "#### Main objective\n",
    "Build a model using PyTorch library that predicts temperature for the next day.\n",
    "<br><br>\n",
    "#### Process includes:\n",
    "1. **Data** <br>\n",
    "&ensp;1.1 Overview <br>\n",
    "&ensp;1.2 Anomalies <br>\n",
    "&ensp;1.3 Visualization <br>\n",
    "&ensp;1.4 Preparing data for model <br>\n",
    "2. **Preparing Dataset** <br>\n",
    "&ensp;2.1 Load data<br>\n",
    "&ensp;2.2 Feature scaling <br>\n",
    "&ensp;2.3 Train / test split <br>\n",
    "&ensp;2.4 Data loader <br>\n",
    "3. **Building Model** <br>\n",
    "&ensp;3.1 Class RNN<br>\n",
    "&ensp;3.2 Training <br>\n",
    "&ensp;3.3 Testing <br>\n",
    "&ensp;3.4 Second model <br>\n",
    "&ensp;3.5a Cross-validation for single feature <br>\n",
    "&ensp;3.5b Cross-validation for multiple features <br>\n",
    "&ensp;3.6 Comparison <br>\n",
    "4. **Conclusion** <br>"
   ],
   "id": "9f988730a9f468c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Setup\n",
    "Matplotlib color scheme."
   ],
   "id": "6a93d4a9fcb6a55e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "WHITE_MID = '#b5b5b5'\n",
    "GREY_DARK = '#141414'\n",
    "\n",
    "plt.rcParams['figure.facecolor'] = GREY_DARK\n",
    "plt.rcParams['text.color'] = WHITE_MID\n",
    "plt.rcParams['axes.facecolor'] = GREY_DARK\n",
    "plt.rcParams['axes.edgecolor'] = WHITE_MID\n",
    "plt.rcParams['axes.labelcolor'] = WHITE_MID\n",
    "plt.rcParams['axes.titlecolor'] = WHITE_MID\n",
    "\n",
    "plt.rcParams['grid.color'] = WHITE_MID\n",
    "plt.rcParams['grid.linestyle'] = '--'\n",
    "plt.rcParams['grid.linewidth'] = 0.5\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['xtick.color'] = WHITE_MID\n",
    "plt.rcParams['ytick.color'] = WHITE_MID\n",
    "plt.rcParams['legend.edgecolor'] = WHITE_MID\n",
    "plt.rcParams['legend.labelcolor'] = WHITE_MID"
   ],
   "id": "f591f0f57656af6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Random seeds, cuda and warnings.",
   "id": "81f3e667b7df5093"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# the model is not reproducible probably due to training on GPU (CUDA)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "id": "e33e1d9469ad1c0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# II - Preparing Dataset\n",
    "#### 2.1 - Load data"
   ],
   "id": "f6676f7817f4eef5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../datasets/weather-processed.csv')\n",
    "df"
   ],
   "id": "e8c4d9b7e5bfddd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.2 - Feature scaling\n",
    "Using _**MinMaxScaler**_ from sklearn."
   ],
   "id": "726118b08d1ac7a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "features = ['precipitation', 'temp_avg', 'wind']\n",
    "\n",
    "df.loc[:, features] = scaler.fit_transform(df[features]) # applying scaled features to numeric features\n",
    "\n",
    "df.describe()"
   ],
   "id": "294bb15e0c7c1fe7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2.3 - Train / test split",
   "id": "d9e285270e10d81d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_size = int(len(df) * 0.7) + 1\n",
    "test_size = int(len(df) * 0.3)\n",
    "\n",
    "print(\n",
    "    f\"{f\"train size\":>12}: {train_size}\",\n",
    "    f\"{f\"test size\":>12}: {test_size}\",\n",
    "    len(df) == train_size + test_size,\n",
    "    sep='\\n'\n",
    ")"
   ],
   "id": "9b2debeeda67b756",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_train = df[:train_size]\n",
    "df_test = df[train_size:]"
   ],
   "id": "7f687d508b204799",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We're going to pass sequenced data with length of n days per sequence. <br>\n",
    "Knowing that, we have to make sure that our datasets are divisible by n. <br><br>\n",
    "With _sequence_len_ parameter we can adjust length of sequence in days (1 day, week, month) and test how it affects models predictions. <br>\n",
    "Despite this there's no enough data to test this model on longer sequences like month, so I chose one week."
   ],
   "id": "fd64df1f598cc281"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sequence_len = 7\n",
    "\n",
    "df_train = df_train[:(len(df_train) // sequence_len) * sequence_len]\n",
    "df_test = df_test[:(len(df_test) // sequence_len) * sequence_len]\n",
    "\n",
    "print(\n",
    "    len(df_train),\n",
    "    len(df_test), sep='\\n'\n",
    ")"
   ],
   "id": "939b33504795111e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Convert each data split to numpy, so we're able to pass it to torch.",
   "id": "14c9b4a8b7765d10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_train_torch = df_train.to_numpy()\n",
    "df_train_torch = torch.tensor(df_train_torch)\n",
    "\n",
    "df_test_torch = df_test.to_numpy()\n",
    "df_test_torch = torch.tensor(df_test_torch)\n",
    "df_test_torch[:7]"
   ],
   "id": "f6de761f5c5ade6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.4 - Data loader\n",
    "To predict temperature based on features from past n days, <br>\n",
    "we have to reshape our data split sets to return specific sequences. <br><br>\n",
    "Each sequence containing data from 7 days, where: <br>\n",
    "X - temp_avg from first n days <br>\n",
    "y - same feature, but we take just the last day"
   ],
   "id": "e90715a5b5a46716"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def sequence_dataset(dataset, batch_size, extra_features=False):\n",
    "    \n",
    "    x, y = [], []\n",
    "    \n",
    "    for i in range(0, len(dataset) - sequence_len, sequence_len):\n",
    "        \n",
    "        # if extra_features = True, then x = [precipitation, wind, temp_avg], y = temp_avg\n",
    "        # if extra_features = False, then x = temp_avg, y = temp_avg\n",
    "        if extra_features:\n",
    "            week = dataset[i: i + sequence_len, :] # take precipitation, wind and temperature (:)\n",
    "            week_input = week[:sequence_len-1] # trim to n days\n",
    "            week_target = week[-1, 2] # take last day\n",
    "        else:\n",
    "            week = dataset[i: i + sequence_len, 2] # take just the temperature (2)\n",
    "            week_input = week[:sequence_len-1] # trim to n days\n",
    "            week_target = week[-1] # take last day\n",
    "        \n",
    "        x.append(week_input)\n",
    "        y.append(week_target)\n",
    "        \n",
    "    # covert to tensor dataset\n",
    "    tensor_dataset = TensorDataset(torch.tensor(np.asarray(x)),\n",
    "                                   torch.tensor(np.asarray(y)))\n",
    "    \n",
    "    # covert to data_loader\n",
    "    data_loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    return data_loader"
   ],
   "id": "f55402288d2398b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 7\n",
    "train_loader = sequence_dataset(df_train_torch, batch_size=batch_size)\n",
    "test_loader = sequence_dataset(df_test_torch, batch_size=batch_size)"
   ],
   "id": "5d1936ec49fb6c44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for batch in train_loader:\n",
    "    input, target = batch\n",
    "    print(input[0], target[0], sep='\\n')\n",
    "    break"
   ],
   "id": "d2c519b521e703a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# III - Building Model\n",
    "#### 3.1 - Class RNN\n",
    "With PyTorch we are able to build simple RNN with Long Short-Term Memory layers by simply attaching _nn.LSTM()_ with specific factors in it. <br>\n",
    "The __init__ function first initializes the LSTM and fully connected layers. <br>\n",
    "__*forward()*__ passes our input x to the rnn network then fully connects it to finally return last value from the output."
   ],
   "id": "9736826d8b8f86f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "class WeatherRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=3, n_layers=2, dropout=0):\n",
    "        super(WeatherRNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :]) # take the last hidden\n",
    "        return out"
   ],
   "id": "a6d6ea3cef0ac0da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.2 - Training\n",
    "So we take our train_loader and pass it to our training function. <br>\n",
    "Training function iterates over batches for n_epoch times and passes sequences from these batches to our model <br>\n",
    "that calculates loss by comparing calculated outputs with actual targets using Mean Squared Error function."
   ],
   "id": "64ddf19a565f0f9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "def train_model(model, device, train_loader, n_epochs=1, lr=0.001, extra_features=False, verbose=1):\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    loss_history = []\n",
    "    best_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_weights = copy.deepcopy(model.state_dict()) # init weights\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = (inputs.to(device).float(),\n",
    "                               targets.to(device).float())\n",
    "            \n",
    "            if not extra_features:\n",
    "                inputs = inputs.unsqueeze(-1)\n",
    "            \n",
    "            optimizer.zero_grad() # clean gradients\n",
    "            outputs = model(inputs) # calculate outputs\n",
    "            \n",
    "            loss = criterion(outputs, targets.unsqueeze(1)) # add dim\n",
    "            loss.backward() # backpropagation\n",
    "            optimizer.step() # update weights\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        # check if current epoch is the best one\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch - 1\n",
    "            \n",
    "        if verbose:\n",
    "            if epoch % 10 == 0 or epoch == 1:\n",
    "                print(f\"Epoch {f\"{epoch}\":>02} / {n_epochs} finished | {epoch_loss/len(train_loader):.4f}\")\n",
    "                \n",
    "        loss_history.append(epoch_loss)\n",
    "        \n",
    "    model.load_state_dict(best_weights)\n",
    "    \n",
    "    return loss_history, best_epoch"
   ],
   "id": "c149884f8a3e3ac7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_size = 1\n",
    "output_size = 1\n",
    "hidden_size = 64\n",
    "n_layers = 1\n",
    "dropout = 0\n",
    "learning_rate = 0.0001\n",
    "n_epochs = 50\n",
    "\n",
    "weather_rnn = WeatherRNN(input_size=input_size, hidden_size=hidden_size, n_layers=n_layers, output_size=output_size, dropout=dropout)\n",
    "\n",
    "loss_history, best_epoch = train_model(weather_rnn, device=device, train_loader=train_loader, n_epochs=n_epochs, lr=learning_rate)"
   ],
   "id": "4b78278af1d420aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sb\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "plt.suptitle('Weather RNN - Loss')\n",
    "\n",
    "sb.lineplot(loss_history, ax=axes[0])\n",
    "axes[0].set_xticks(range(0, n_epochs + 1, 2))\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].axvline(x=best_epoch, color='g', linestyle=':')\n",
    "\n",
    "loss_history_rescaled = np.zeros((len(loss_history), 3))\n",
    "loss_history_rescaled[:, 2] = np.array(loss_history)\n",
    "loss_history_rescaled = scaler.inverse_transform(loss_history_rescaled)[:, 2]\n",
    "\n",
    "sb.lineplot(loss_history_rescaled, ax=axes[1])\n",
    "axes[1].set_xticks(range(0, n_epochs + 1, 2))\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss (Temperature)')\n",
    "axes[1].set_ylim((0, 5))\n",
    "axes[1].axvline(x=best_epoch, color='g', linestyle=':')\n",
    "\n",
    "plt.tight_layout()"
   ],
   "id": "4d05cfdfe602a716",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.3 - Testing\n",
    "Similar to training function. This time we run the model with no gradients calculations <br>\n",
    "and iterate through batches from test_loader. <br>\n",
    "Then return y true and y predicted values to compare where the model makes most mistakes."
   ],
   "id": "f2c8ce9e9d98917d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# rescaling score data back to temperature format.\n",
    "def to_degrees(feature):\n",
    "    feature_deg = np.zeros((len(feature), 3)) # because our scaler remember what did it scale before, we need to create data with same dimensions\n",
    "    feature_deg[:, 2] = np.sqrt(feature) # square root, because we use Mean Square Error in our models\n",
    "    feature_deg = scaler.inverse_transform(feature_deg)\n",
    "    return feature_deg[:, 2]"
   ],
   "id": "bf92939259d239cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test_model(model, device, test_loader, extra_features=False):\n",
    "    total_loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = (inputs.to(device).float(),\n",
    "                               targets.to(device).float())\n",
    "            \n",
    "            if not extra_features:\n",
    "                inputs = inputs.unsqueeze(-1)\n",
    "\n",
    "            outputs = model(inputs) # get outputs\n",
    "\n",
    "            loss = criterion(outputs, targets.unsqueeze(1)) # add dim\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            y_true.extend(targets.cpu().numpy())\n",
    "            y_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(f\"Test MSE Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return y_true, y_pred, avg_loss"
   ],
   "id": "e03eaec27e834ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sb\n",
    "\n",
    "y_true, y_pred, mse = test_model(weather_rnn, device=device, test_loader=test_loader)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(24, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "temp_avg_true = np.array(y_true).flatten()\n",
    "temp_avg_pred = np.array(y_pred).flatten()\n",
    "\n",
    "x = range(len(temp_avg_true))\n",
    "axes[0].vlines(x=x, ymin=temp_avg_pred, ymax=temp_avg_true, color='red', linewidth=1, label='error')\n",
    "axes[0].plot(x, y_pred, marker='+', linestyle='None', color='white', label='pred')\n",
    "\n",
    "sb.lineplot(x=range(len(temp_avg_true)), y=temp_avg_true, color='red', ax=axes[1], label='true')\n",
    "sb.lineplot(x=range(len(temp_avg_pred)), y=temp_avg_pred, color='white', ax=axes[1], label='pred')\n",
    "\n",
    "axes[0].set_title('Predictions Errors')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_title('True and Predicted overlap')\n",
    "\n",
    "plt.suptitle('Predictions Summary', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "true_deg = to_degrees(temp_avg_true)\n",
    "pred_deg = to_degrees(temp_avg_pred)\n",
    "\n",
    "error = abs(true_deg - pred_deg)\n",
    "\n",
    "print(\n",
    "    \"\\nError in degrees:\\n\"\n",
    "    f\"{\"mean\":<6}: {error.mean():4f}\",\n",
    "    f\"{\"max\":<6}: {error.max():4f}\",\n",
    "    f\"{\"min\":<6}: {error.min():4f}\",\n",
    "    f\"{\"range\":<6}: {(true_deg.max() - true_deg.min()):4f}\",\n",
    "    sep='\\n'\n",
    ")"
   ],
   "id": "d7a4bcf54377ae33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.4 - Second model\n",
    "Now we use 3 features (precipitation, wind, temp_avg) to predict just a temp_avg.\n"
   ],
   "id": "c91a75544c8037f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_loader_2 = sequence_dataset(df_train_torch, batch_size=batch_size, extra_features=True)\n",
    "test_loader_2 = sequence_dataset(df_test_torch, batch_size=batch_size, extra_features=True)"
   ],
   "id": "dac9cf2ea6dffe80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "i = 0\n",
    "for batch in train_loader_2:\n",
    "    if i == 0:\n",
    "        input, target = batch\n",
    "        # print(input, target)\n",
    "        print(input.size())\n",
    "        print(target.size())\n",
    "        break\n",
    "    i += 1"
   ],
   "id": "4606cc569d44664c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "weather_rnn_2 = WeatherRNN(input_size=3,\n",
    "                           hidden_size=hidden_size,\n",
    "                           n_layers=n_layers,\n",
    "                           output_size=output_size,\n",
    "                           dropout=dropout)\n",
    "\n",
    "loss_history_2, best_epoch_2 = train_model(weather_rnn_2, device=device, train_loader=train_loader_2, n_epochs=n_epochs, lr=learning_rate, extra_features=True)"
   ],
   "id": "5a8d4cfb9b7d7635",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_true_2, y_pred_2, mse_2 = test_model(weather_rnn_2, device=device, test_loader=test_loader_2, extra_features=True)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(24, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "temp_avg_true_2 = np.array(y_true_2).flatten()\n",
    "temp_avg_pred_2 = np.array(y_pred_2).flatten()\n",
    "\n",
    "x = range(len(temp_avg_true_2))\n",
    "axes[0].vlines(x=x, ymin=temp_avg_pred_2, ymax=temp_avg_true_2, color='red', linewidth=1, label='error')\n",
    "axes[0].plot(x, y_pred_2, marker='+', linestyle='None', color='white', label='pred')\n",
    "\n",
    "sb.lineplot(x=range(len(temp_avg_true_2)), y=temp_avg_true_2, color='red', ax=axes[1], label='true')\n",
    "sb.lineplot(x=range(len(temp_avg_pred_2)), y=temp_avg_pred_2, color='white', ax=axes[1], label='pred')\n",
    "\n",
    "axes[0].set_title('Prediction errors')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_title('True and Predicted values overlap')\n",
    "\n",
    "plt.suptitle('Predictions Summary', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "true_deg_2 = to_degrees(temp_avg_true_2)\n",
    "pred_deg_2 = to_degrees(temp_avg_pred_2)\n",
    "\n",
    "error_2 = abs(true_deg_2 - pred_deg_2)\n",
    "\n",
    "print(\n",
    "    \"\\nError in degrees:\\n\"\n",
    "    f\"{\"mean\":<6}: {error_2.mean():4f}\",\n",
    "    f\"{\"max\":<6}: {error_2.max():4f}\",\n",
    "    f\"{\"min\":<6}: {error_2.min():4f}\",\n",
    "    f\"{\"range\":<6}: {(true_deg_2.max() - true_deg_2.min()):4f}\",\n",
    "    sep='\\n'\n",
    ")"
   ],
   "id": "59ef2b9b605d6edf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.5a - Cross-validation for single feature\n",
    "To test the model on the whole dataset without overfitting we can commit _**Cross-Validation**_."
   ],
   "id": "7b191799a1689fb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# n_folds is number of chunks we split our dataset for\n",
    "n_folds = 5\n",
    "df = df.iloc[: (len(df) // n_folds) * n_folds]\n",
    "step = len(df) // n_folds\n",
    "\n",
    "df_splits = []\n",
    "\n",
    "for i in range(0, len(df), step):\n",
    "    test_chunk = df.iloc[i: i+step] # select certain chunk as test_chunk\n",
    "    train_chunk = df.drop(df[i: i+step].index) # and then remove the same chunk from train_chunk\n",
    "    df_splits.append((train_chunk, test_chunk))"
   ],
   "id": "94b49824325bc9be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "cross_val_errors_1 = [] # a list that contains all errors for single feature model for each fold\n",
    "\n",
    "# cross-validation\n",
    "for i, split in enumerate(df_splits):\n",
    "    train, test = split\n",
    "    \n",
    "    # train loader\n",
    "    train_torch = train.to_numpy()\n",
    "    train_torch = torch.tensor(train_torch)\n",
    "    train_loader = sequence_dataset(train_torch, batch_size=batch_size, extra_features=False)\n",
    "    \n",
    "    # test loader\n",
    "    test_torch = test.to_numpy()\n",
    "    test_torch = torch.tensor(test_torch)\n",
    "    test_loader = sequence_dataset(test_torch, batch_size=batch_size, extra_features=False)\n",
    "    \n",
    "    nth_model = WeatherRNN(input_size=1, hidden_size=hidden_size, n_layers=n_layers, output_size=output_size, dropout=dropout) # define model\n",
    "    \n",
    "    print(f'Fold {i+1} - ', end='')\n",
    "    train_model(nth_model, device=device, train_loader=train_loader, n_epochs=n_epochs, lr=learning_rate, extra_features=False, verbose=0) # train model\n",
    "    \n",
    "    y_true, y_pred, mse = test_model(nth_model, device=device, test_loader=test_loader, extra_features=False) # test model\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    error = abs(y_true - y_pred) # absolute value of prediction errors\n",
    "\n",
    "    cross_val_errors_1.append(error)\n",
    "    \n",
    "print(f\"MSE Loss for {n_folds} folds: {mse:4f}\")\n",
    "print(f\"Error for {n_folds} folds: {np.mean(cross_val_errors_1):4f}\")\n",
    "print(f\"Global MAE for {n_folds} folds: {mean_absolute_error():4f}\")"
   ],
   "id": "83844e56f7959d2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.5b Cross-validation for multiple features\n",
    "To test the model with multiple features in cross-validation we simply change _**extra_features**_ boolean value to True in: <br>\n",
    "_train_loader_, _test_loader_, _train_model_ and _test_model_. <br>\n",
    "This will pass all the features (precipitation, wind, temp_avg) to all functions above."
   ],
   "id": "3e049e30f24b7bc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.mean([0.0070, 0.0082, 0.0082, 0.0099, 0.0100])",
   "id": "f45e0303fcedfd63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cross_val_errors_2 = [] # a list that contains all errors for multiple feature model for each fold\n",
    "\n",
    "# cross-validation\n",
    "for i, split in enumerate(df_splits):\n",
    "    train, test = split\n",
    "\n",
    "    # train loader\n",
    "    train_torch = train.to_numpy()\n",
    "    train_torch = torch.tensor(train_torch)\n",
    "    train_loader = sequence_dataset(train_torch, batch_size=batch_size, extra_features=True)\n",
    "\n",
    "    # test loader\n",
    "    test_torch = test.to_numpy()\n",
    "    test_torch = torch.tensor(test_torch)\n",
    "    test_loader = sequence_dataset(test_torch, batch_size=batch_size, extra_features=True)\n",
    "\n",
    "    nth_model = WeatherRNN(input_size=3, hidden_size=hidden_size, n_layers=n_layers, output_size=output_size, dropout=dropout) # define model\n",
    "\n",
    "    print(f'Fold {i+1}:')\n",
    "    train_model(nth_model, device=device, train_loader=train_loader, n_epochs=n_epochs, lr=learning_rate, extra_features=True) # train model\n",
    "\n",
    "    y_true, y_pred = test_model(nth_model, device=device, test_loader=test_loader, extra_features=True) # test model\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "    error = abs(y_true - y_pred) # absolute value of prediction errors\n",
    "    print('')\n",
    "\n",
    "    cross_val_errors_2.append(error)\n",
    "\n",
    "print(f\"Average error for {n_folds} folds: {np.mean(cross_val_errors_2):4f}\")"
   ],
   "id": "2807c043e264361",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.6 - Comparison\n",
    "Now we create one big DataFrame that contains all information that cross-validation produced."
   ],
   "id": "bc1ac36dfbae59ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "records = []\n",
    "\n",
    "# n_features stands for features used to predict temperature (e.g. n_features=3: precip, wind, temp -> temp)\n",
    "for n_features, data in zip([1, 3], [cross_val_errors_1, cross_val_errors_2]):\n",
    "    data = np.stack(data)\n",
    "    for fold in range(n_folds):\n",
    "        fold_data = data[fold] # take certain data by fold idx\n",
    "        records.append({\n",
    "            'n_features': n_features,\n",
    "            'fold': str(fold),\n",
    "            'mean': fold_data.mean(),\n",
    "            'min': fold_data.min(),\n",
    "            'max': fold_data.max(),\n",
    "            'errors': fold_data[fold]\n",
    "        })\n",
    "\n",
    "cross_val_df = pd.DataFrame(records)"
   ],
   "id": "14bd246205ae2469",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cross_val_df['mean'].max()",
   "id": "a6a98aa46c6850a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cross_val_df['min'].min()",
   "id": "61ca76a18e02c89a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "model_1 = cross_val_df.n_features == 1\n",
    "model_3 = cross_val_df.n_features == 3\n",
    "\n",
    "sb.barplot(data=cross_val_df, x='mean', y='fold', hue='n_features', palette=\"muted\", linewidth=0, ax=axes[0])\n",
    "sb.barplot(data=cross_val_df, x='min', y='fold', hue='n_features', palette=\"muted\", linewidth=0, ax=axes[1])\n",
    "sb.barplot(data=cross_val_df, x='max', y='fold', hue='n_features', palette=\"muted\", linewidth=0, ax=axes[2])\n",
    "\n",
    "axes[0].set_xlim(cross_val_df['mean'].min() - .005, cross_val_df['mean'].max() + .005)\n",
    "axes[1].set_xlim(cross_val_df['min'].min() - .005, cross_val_df['min'].max() + .005)\n",
    "axes[2].set_xlim(cross_val_df['max'].min() - .005, cross_val_df['max'].max() + .005)\n",
    "\n",
    "plt.show()"
   ],
   "id": "88b43450a9c927d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# V - Conclusion\n",
    "The chaos in the _precipitation_ and _wind_ series doesn't seem to provide any valuable information that helps the model predict the average temperature. <br>\n",
    "Despite this fact the model did decent job with predicting average temperature, both for weekly and daily sequences. <br><br>\n",
    "Overall I think this project gives quiet simple fundamentals to understand the functionality of Recurrent Neural Networks.\n",
    "#### Thank you for reading my project, I hope you enjoyed the process :]\n",
    "_- Gracjan Paw≈Çowski, 2025_"
   ],
   "id": "66e02b2ff3215bed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
